{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Delivery - Hyperparameter Tuning\n",
    "## Multilayer Perceptron Tuning\n",
    "The aim of this fourth delivery will be to optimize properly the multilayer perceptron.\n",
    "\n",
    "**First**\n",
    "After preprocessing the data, we use a grid search in order to find the most optimized parameters. We use the precision_score (tp / (tp + fp)) as the main measure. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5249 entries, 0 to 5248\n",
      "Columns: 521 entries, WAP001 to ID\n",
      "dtypes: int64(521)\n",
      "memory usage: 20.9 MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../datasets/UJIIndoorLoc/UJIIndoorLoc_B0-ID-01.csv\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAP001</th>\n",
       "      <th>WAP002</th>\n",
       "      <th>WAP003</th>\n",
       "      <th>WAP004</th>\n",
       "      <th>WAP005</th>\n",
       "      <th>WAP006</th>\n",
       "      <th>WAP007</th>\n",
       "      <th>WAP008</th>\n",
       "      <th>WAP009</th>\n",
       "      <th>WAP010</th>\n",
       "      <th>...</th>\n",
       "      <th>WAP512</th>\n",
       "      <th>WAP513</th>\n",
       "      <th>WAP514</th>\n",
       "      <th>WAP515</th>\n",
       "      <th>WAP516</th>\n",
       "      <th>WAP517</th>\n",
       "      <th>WAP518</th>\n",
       "      <th>WAP519</th>\n",
       "      <th>WAP520</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 521 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   WAP010  ...  WAP512  WAP513  WAP514  WAP515  WAP516  WAP517  WAP518  \\\n",
       "0       0  ...       0       0       0       0       0       0       0   \n",
       "1       0  ...       0       0       0       0       0       0       0   \n",
       "2       0  ...       0       0       0       0       0       0       0   \n",
       "3       0  ...       0       0       0       0       0       0       0   \n",
       "4       0  ...       0       0       0       0       0       0       0   \n",
       "\n",
       "   WAP519  WAP520   ID  \n",
       "0       0       0  122  \n",
       "1       0       0  102  \n",
       "2       0       0  110  \n",
       "3       0       0  111  \n",
       "4       0       0  107  \n",
       "\n",
       "[5 rows x 521 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=9)\n",
    "param_grid = {'solver': ['adam', 'sgd','lbfgs'],\n",
    "              'activation':['logistic','relu'],\n",
    "              'max_iter': [5,10,15], \n",
    "              'alpha': 10.0 ** -np.arange(1, 4), \n",
    "              'hidden_layer_sizes':np.arange(20, 50), \n",
    "              #'random_state':[0,1,2,3,4,5,6,7,8,9]\n",
    "             }\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score, average = 'weighted'),\n",
    "    'roc_auc_score': make_scorer(roc_auc_score),\n",
    "#    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter = sys.maxsize, verbose=10)\n",
    "# mlp_grid = GridSearchCV(mlp, param_grid, cv=skf, scoring = scorers, refit = make_scorer(precision_score, average = 'weighted'), n_jobs=-1)\n",
    "mlp_grid = GridSearchCV(mlp, param_grid, cv=skf, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.51295014\n",
      "Iteration 2, loss = 5.34888041\n",
      "Iteration 3, loss = 5.04538180\n",
      "Iteration 4, loss = 4.67464477\n",
      "Iteration 5, loss = 4.31212252\n",
      "Iteration 6, loss = 3.97678428\n",
      "Iteration 7, loss = 3.67958275\n",
      "Iteration 8, loss = 3.42439972\n",
      "Iteration 9, loss = 3.20821191\n",
      "Iteration 10, loss = 3.02230075\n",
      "Iteration 11, loss = 2.85480534\n",
      "Iteration 12, loss = 2.70996067\n",
      "Iteration 13, loss = 2.58041978\n",
      "Iteration 14, loss = 2.46579184\n",
      "Iteration 15, loss = 2.35929215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnau/anaconda3/envs/scii/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "mlp_grid.fit(X_train, y_train)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting time:3011.917\n"
     ]
    }
   ],
   "source": [
    "fit_time = round(t2-t1, 3)\n",
    "print(\"Fitting time:{}\".format(fit_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.01,\n",
       " 'hidden_layer_sizes': 49,\n",
       " 'max_iter': 15,\n",
       " 'solver': 'adam'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_grid.best_params_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "mlp_grid.best_params_\n",
    "\n",
    "{'activation': 'relu',\n",
    " 'alpha': 0.01,\n",
    " 'hidden_layer_sizes': 4,\n",
    " 'max_iter': 15,\n",
    " 'random_state': 8,\n",
    " 'solver': 'lbfgs'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp_grid.predict(X_test)\n",
    "y_pred_prob = mlp_grid.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  99.22874562903961\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC: \", auc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  47.0476\n"
     ]
    }
   ],
   "source": [
    "# print(\"Accuracy: \", round(100 * accuracy_score(y_test, y_pred), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         102       0.00      0.00      0.00         2\n",
      "         106       1.00      1.00      1.00         1\n",
      "         107       0.00      0.00      0.00         1\n",
      "         110       1.00      1.00      1.00         1\n",
      "         111       0.00      0.00      0.00         2\n",
      "         112       0.00      0.00      0.00         2\n",
      "         113       1.00      0.50      0.67         2\n",
      "         114       0.00      0.00      0.00         2\n",
      "         115       1.00      1.00      1.00         1\n",
      "         116       0.50      1.00      0.67         1\n",
      "         117       0.00      0.00      0.00         1\n",
      "         118       0.00      0.00      0.00         1\n",
      "         119       1.00      1.00      1.00         2\n",
      "         120       0.14      1.00      0.25         4\n",
      "         121       0.00      0.00      0.00         2\n",
      "         122       0.00      0.00      0.00         2\n",
      "         123       0.00      0.00      0.00         2\n",
      "         125       0.00      0.00      0.00         2\n",
      "         126       1.00      0.50      0.67         2\n",
      "         127       0.00      0.00      0.00         2\n",
      "         128       0.29      0.67      0.40         3\n",
      "         129       0.00      0.00      0.00         2\n",
      "         130       0.00      0.00      0.00         2\n",
      "         131       0.00      0.00      0.00         1\n",
      "         132       1.00      0.50      0.67         2\n",
      "         133       0.50      0.67      0.57         3\n",
      "         134       0.00      0.00      0.00         3\n",
      "         201       1.00      0.50      0.67         2\n",
      "         202       0.67      1.00      0.80         2\n",
      "         208       0.00      0.00      0.00         2\n",
      "         209       1.00      1.00      1.00         2\n",
      "         211       0.25      0.50      0.33         2\n",
      "         212       0.00      0.00      0.00         2\n",
      "         213       1.00      0.50      0.67         2\n",
      "         214       0.50      0.50      0.50         2\n",
      "         215       0.50      0.50      0.50         2\n",
      "         216       1.00      0.50      0.67         2\n",
      "         218       0.67      1.00      0.80         2\n",
      "         219       0.00      0.00      0.00         2\n",
      "         220       0.50      0.50      0.50         2\n",
      "         222       0.00      0.00      0.00         2\n",
      "         224       0.00      0.00      0.00         2\n",
      "         225       0.33      1.00      0.50         2\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.50      0.50      0.50         2\n",
      "         229       0.00      0.00      0.00         2\n",
      "         230       0.67      1.00      0.80         2\n",
      "         231       0.67      1.00      0.80         2\n",
      "         232       1.00      0.50      0.67         2\n",
      "         233       0.50      1.00      0.67         2\n",
      "         234       0.00      0.00      0.00         2\n",
      "         235       0.25      0.50      0.33         2\n",
      "         236       0.50      1.00      0.67         2\n",
      "         237       0.20      0.50      0.29         2\n",
      "        1101       0.00      0.00      0.00         2\n",
      "        1102       0.33      0.67      0.44         3\n",
      "        1103       0.50      0.67      0.57         3\n",
      "        1104       0.00      0.00      0.00         3\n",
      "        1105       0.40      0.67      0.50         3\n",
      "        1106       0.50      0.67      0.57         3\n",
      "        1107       0.67      0.67      0.67         3\n",
      "        1108       0.50      1.00      0.67         3\n",
      "        1109       0.40      0.67      0.50         3\n",
      "        1110       0.00      0.00      0.00         2\n",
      "        1111       1.00      0.50      0.67         2\n",
      "        1112       0.50      0.50      0.50         2\n",
      "        1113       0.00      0.00      0.00         2\n",
      "        1114       0.00      0.00      0.00         2\n",
      "        1115       0.00      0.00      0.00         2\n",
      "        1116       1.00      0.50      0.67         2\n",
      "        1117       1.00      0.50      0.67         2\n",
      "        1118       1.00      0.50      0.67         2\n",
      "        1119       0.50      0.50      0.50         2\n",
      "        1120       0.33      0.50      0.40         2\n",
      "        1121       0.00      0.00      0.00         2\n",
      "        1122       1.00      0.50      0.67         2\n",
      "        1123       0.00      0.00      0.00         2\n",
      "        1124       1.00      0.50      0.67         2\n",
      "        1125       0.25      1.00      0.40         2\n",
      "        1126       0.50      0.50      0.50         2\n",
      "        1127       0.00      0.00      0.00         1\n",
      "        1128       0.67      1.00      0.80         2\n",
      "        1129       0.00      0.00      0.00         1\n",
      "        1130       0.67      1.00      0.80         2\n",
      "        1136       1.00      1.00      1.00         1\n",
      "        1137       0.00      0.00      0.00         1\n",
      "        1138       1.00      0.50      0.67         2\n",
      "        1201       0.00      0.00      0.00         2\n",
      "        1202       0.33      0.50      0.40         2\n",
      "        1203       0.40      1.00      0.57         2\n",
      "        1204       0.00      0.00      0.00         2\n",
      "        1205       0.30      1.00      0.46         3\n",
      "        1206       0.00      0.00      0.00         2\n",
      "        1207       0.00      0.00      0.00         2\n",
      "        1208       0.00      0.00      0.00         2\n",
      "        1209       0.50      0.50      0.50         2\n",
      "        1210       0.00      0.00      0.00         2\n",
      "        1211       1.00      0.50      0.67         2\n",
      "        1212       0.00      0.00      0.00         1\n",
      "        1213       0.25      0.50      0.33         2\n",
      "        1214       0.50      0.50      0.50         2\n",
      "        1215       0.00      0.00      0.00         2\n",
      "        1216       0.00      0.00      0.00         2\n",
      "        1217       0.33      1.00      0.50         3\n",
      "        1218       1.00      0.50      0.67         2\n",
      "        1219       0.23      1.00      0.38         3\n",
      "        1220       0.00      0.00      0.00         2\n",
      "        1221       0.00      0.00      0.00         2\n",
      "        1222       0.00      0.00      0.00         2\n",
      "        1223       0.50      0.50      0.50         2\n",
      "        1224       0.00      0.00      0.00         2\n",
      "        1225       0.00      0.00      0.00         2\n",
      "        1226       0.00      0.00      0.00         2\n",
      "        1227       0.00      0.00      0.00         1\n",
      "        1228       1.00      1.00      1.00         2\n",
      "        1229       1.00      1.00      1.00         2\n",
      "        1230       1.00      1.00      1.00         2\n",
      "        1233       1.00      0.50      0.67         2\n",
      "        1234       0.50      1.00      0.67         2\n",
      "        1235       0.00      0.00      0.00         2\n",
      "        2101       0.00      0.00      0.00         2\n",
      "        2102       0.25      0.67      0.36         3\n",
      "        2103       0.67      1.00      0.80         2\n",
      "        2104       1.00      0.50      0.67         2\n",
      "        2105       0.38      1.00      0.55         3\n",
      "        2106       1.00      1.00      1.00         2\n",
      "        2107       0.25      0.50      0.33         2\n",
      "        2108       0.00      0.00      0.00         2\n",
      "        2109       1.00      0.50      0.67         2\n",
      "        2110       0.50      0.50      0.50         2\n",
      "        2111       0.00      0.00      0.00         2\n",
      "        2112       0.50      1.00      0.67         2\n",
      "        2113       0.00      0.00      0.00         2\n",
      "        2114       0.00      0.00      0.00         2\n",
      "        2115       0.00      0.00      0.00         2\n",
      "        2117       0.25      0.50      0.33         2\n",
      "        2118       0.00      0.00      0.00         2\n",
      "        2119       0.50      1.00      0.67         2\n",
      "        2120       0.00      0.00      0.00         2\n",
      "        2121       0.00      0.00      0.00         2\n",
      "        2122       0.67      0.67      0.67         3\n",
      "        2123       0.50      1.00      0.67         2\n",
      "        2124       1.00      0.50      0.67         2\n",
      "        2125       0.00      0.00      0.00         2\n",
      "        2126       0.14      0.50      0.22         2\n",
      "        2127       0.00      0.00      0.00         2\n",
      "        2128       0.50      0.67      0.57         3\n",
      "        2129       1.00      0.50      0.67         2\n",
      "        2130       0.00      0.00      0.00         1\n",
      "        2132       1.00      1.00      1.00         2\n",
      "        2133       1.00      1.00      1.00         2\n",
      "        2134       1.00      1.00      1.00         2\n",
      "        2138       0.67      1.00      0.80         2\n",
      "        2139       0.00      0.00      0.00         2\n",
      "        2140       0.33      0.33      0.33         3\n",
      "        2201       0.00      0.00      0.00         2\n",
      "        2202       1.00      1.00      1.00         2\n",
      "        2203       1.00      0.50      0.67         2\n",
      "        2204       0.00      0.00      0.00         2\n",
      "        2205       1.00      0.50      0.67         2\n",
      "        2206       0.00      0.00      0.00         2\n",
      "        2207       0.00      0.00      0.00         2\n",
      "        2208       0.20      0.50      0.29         2\n",
      "        2209       1.00      1.00      1.00         2\n",
      "        2210       0.67      1.00      0.80         2\n",
      "        2211       1.00      0.50      0.67         2\n",
      "        2212       0.20      0.50      0.29         2\n",
      "        2213       0.50      0.50      0.50         2\n",
      "        2214       0.40      0.67      0.50         3\n",
      "        2216       0.00      0.00      0.00         2\n",
      "        2217       0.50      0.50      0.50         2\n",
      "        2218       0.43      1.00      0.60         3\n",
      "        2219       0.67      1.00      0.80         2\n",
      "        2220       0.50      0.50      0.50         2\n",
      "        2221       1.00      0.50      0.67         2\n",
      "        2222       0.00      0.00      0.00         2\n",
      "        2223       0.29      0.67      0.40         3\n",
      "        2224       0.00      0.00      0.00         2\n",
      "        2225       0.00      0.00      0.00         2\n",
      "        2226       0.67      1.00      0.80         2\n",
      "        2227       0.00      0.00      0.00         2\n",
      "        2228       0.00      0.00      0.00         2\n",
      "        2229       0.00      0.00      0.00         2\n",
      "        2230       1.00      1.00      1.00         2\n",
      "        2231       1.00      0.67      0.80         3\n",
      "        2234       0.00      0.00      0.00         2\n",
      "        2235       1.00      0.50      0.67         2\n",
      "        2241       0.40      1.00      0.57         2\n",
      "        3101       0.00      0.00      0.00         2\n",
      "        3102       0.33      0.50      0.40         2\n",
      "        3103       1.00      0.50      0.67         2\n",
      "        3104       0.00      0.00      0.00         2\n",
      "        3105       0.00      0.00      0.00         2\n",
      "        3106       0.00      0.00      0.00         2\n",
      "        3107       0.00      0.00      0.00         2\n",
      "        3108       0.00      0.00      0.00         2\n",
      "        3109       1.00      0.50      0.67         2\n",
      "        3110       0.67      1.00      0.80         2\n",
      "        3111       0.40      1.00      0.57         2\n",
      "        3112       0.50      0.50      0.50         2\n",
      "        3113       0.29      1.00      0.44         2\n",
      "        3114       0.00      0.00      0.00         2\n",
      "        3115       1.00      0.50      0.67         2\n",
      "        3116       1.00      0.50      0.67         2\n",
      "        3117       0.00      0.00      0.00         2\n",
      "        3118       1.00      1.00      1.00         2\n",
      "        3119       0.00      0.00      0.00         2\n",
      "        3120       0.67      1.00      0.80         2\n",
      "        3121       1.00      1.00      1.00         2\n",
      "        3122       0.00      0.00      0.00         2\n",
      "        3123       0.50      0.50      0.50         2\n",
      "        3124       0.50      1.00      0.67         2\n",
      "        3125       0.33      0.50      0.40         2\n",
      "        3126       0.33      0.50      0.40         2\n",
      "        3127       0.33      0.50      0.40         2\n",
      "        3128       0.00      0.00      0.00         2\n",
      "        3129       1.00      1.00      1.00         2\n",
      "        3130       1.00      1.00      1.00         2\n",
      "        3131       1.00      1.00      1.00         3\n",
      "        3135       1.00      1.00      1.00         2\n",
      "        3136       0.50      0.50      0.50         2\n",
      "        3137       1.00      1.00      1.00         2\n",
      "        3201       1.00      1.00      1.00         3\n",
      "        3202       0.50      0.50      0.50         2\n",
      "        3203       0.25      0.50      0.33         2\n",
      "        3204       1.00      0.50      0.67         2\n",
      "        3205       0.25      0.50      0.33         2\n",
      "        3206       1.00      0.50      0.67         2\n",
      "        3207       0.00      0.00      0.00         2\n",
      "        3208       0.33      1.00      0.50         2\n",
      "        3209       1.00      0.50      0.67         2\n",
      "        3210       0.33      0.50      0.40         2\n",
      "        3211       1.00      0.50      0.67         2\n",
      "        3212       0.00      0.00      0.00         2\n",
      "        3213       0.00      0.00      0.00         2\n",
      "        3214       1.00      1.00      1.00         1\n",
      "        3215       0.50      0.50      0.50         2\n",
      "        3216       1.00      0.50      0.67         2\n",
      "        3217       0.00      0.00      0.00         2\n",
      "        3218       0.67      0.67      0.67         3\n",
      "        3219       0.00      0.00      0.00         2\n",
      "        3220       0.00      0.00      0.00         2\n",
      "        3221       0.00      0.00      0.00         2\n",
      "        3222       0.38      1.00      0.55         3\n",
      "        3223       1.00      1.00      1.00         2\n",
      "        3224       1.00      0.50      0.67         2\n",
      "        3225       1.00      1.00      1.00         2\n",
      "        3226       1.00      0.50      0.67         2\n",
      "        3227       0.29      1.00      0.44         2\n",
      "        3228       1.00      0.50      0.67         2\n",
      "        3229       0.00      0.00      0.00         2\n",
      "        3230       0.50      0.50      0.50         2\n",
      "        3231       0.33      0.50      0.40         2\n",
      "        3234       0.67      1.00      0.80         2\n",
      "        3235       1.00      0.50      0.67         2\n",
      "        3236       0.43      1.00      0.60         3\n",
      "\n",
      "    accuracy                           0.47       525\n",
      "   macro avg       0.42      0.45      0.40       525\n",
      "weighted avg       0.43      0.47      0.41       525\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnau/anaconda3/envs/scii/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
